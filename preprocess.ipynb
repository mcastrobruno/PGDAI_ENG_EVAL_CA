{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN35xfVyWDaPImK0UjBkUz2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"zNcUx_vgLm38"},"outputs":[],"source":["import pandas as pd \n","import re\n","from Config import *\n","\n","\n","def get_input_data()->pd.DataFrame:\n","    df = pd.read_csv(\"data//AppGallery.csv\", skipinitialspace=True)\n","    df.rename(columns={'Type 1': 'y1', 'Type 2': 'y2', 'Type 3': 'y3', 'Type 4': 'y4'}, inplace=True)\n","    df[Config.INTERACTION_CONTENT] = df[Config.INTERACTION_CONTENT].values.astype('U')\n","    df[Config.TICKET_SUMMARY] = df[Config.TICKET_SUMMARY].values.astype('U')\n","    df[\"y\"] = df[Config.CLASS_COL]\n","    df = df.loc[(df[\"y\"] != '') & (~df[\"y\"].isna()),]\n","    return df\n","\n","def de_duplication(data: pd.DataFrame):\n","    data[\"ic_deduplicated\"] = \"\"\n","\n","    cu_template = {\n","        \"english\":\n","            [\"(?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) Customer Support team\\,?\",\n","             \"(?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) SE is a company incorporated under the laws of Ireland with its headquarters in Dublin, Ireland\\.?\",\n","             \"(?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) SE is the provider of Huawei Mobile Services to Huawei and Honor device owners in (?:Europe|\\*\\*\\*\\*\\*\\(LOC\\)), Canada, Australia, New Zealand and other countries\\.?\"]\n","        ,\n","        \"german\":\n","            [\"(?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) Kundenservice\\,?\",\n","             \"Die (?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) SE ist eine Gesellschaft nach irischem Recht mit Sitz in Dublin, Irland\\.?\",\n","             \"(?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) SE ist der Anbieter von Huawei Mobile Services für Huawei- und Honor-Gerätebesitzer in Europa, Kanada, Australien, Neuseeland und anderen Ländern\\.?\"]\n","        ,\n","        \"french\":\n","            [\"L'équipe d'assistance à la clientèle d'Aspiegel\\,?\",\n","             \"Die (?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) SE est une société de droit irlandais dont le siège est à Dublin, en Irlande\\.?\",\n","             \"(?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) SE est le fournisseur de services mobiles Huawei aux propriétaires d'appareils Huawei et Honor en Europe, au Canada, en Australie, en Nouvelle-Zélande et dans d'autres pays\\.?\"]\n","        ,\n","        \"spanish\":\n","            [\"(?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) Soporte Servicio al Cliente\\,?\",\n","             \"Die (?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) es una sociedad constituida en virtud de la legislación de Irlanda con su sede en Dublín, Irlanda\\.?\",\n","             \"(?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) SE es el proveedor de servicios móviles de Huawei a los propietarios de dispositivos de Huawei y Honor en Europa, Canadá, Australia, Nueva Zelanda y otros países\\.?\"]\n","        ,\n","        \"italian\":\n","            [\"Il tuo team ad (?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)),?\",\n","             \"Die (?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) SE è una società costituita secondo le leggi irlandesi con sede a Dublino, Irlanda\\.?\",\n","             \"(?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) SE è il fornitore di servizi mobili Huawei per i proprietari di dispositivi Huawei e Honor in Europa, Canada, Australia, Nuova Zelanda e altri paesi\\.?\"]\n","        ,\n","        \"portguese\":\n","            [\"(?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) Customer Support team,?\",\n","             \"Die (?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) SE é uma empresa constituída segundo as leis da Irlanda, com sede em Dublin, Irlanda\\.?\",\n","             \"(?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) SE é o provedor de Huawei Mobile Services para Huawei e Honor proprietários de dispositivos na Europa, Canadá, Austrália, Nova Zelândia e outros países\\.?\"]\n","        ,\n","    }\n","\n","    cu_pattern = \"\"\n","    for i in sum(list(cu_template.values()), []):\n","        cu_pattern = cu_pattern + f\"({i})|\"\n","    cu_pattern = cu_pattern[:-1]\n","\n","    # -------- email split template\n","\n","    pattern_1 = \"(From\\s?:\\s?xxxxx@xxxx.com Sent\\s?:.{30,70}Subject\\s?:)\"\n","    pattern_2 = \"(On.{30,60}wrote:)\"\n","    pattern_3 = \"(Re\\s?:|RE\\s?:)\"\n","    pattern_4 = \"(\\*\\*\\*\\*\\*\\(PERSON\\) Support issue submit)\"\n","    pattern_5 = \"(\\s?\\*\\*\\*\\*\\*\\(PHONE\\))*$\"\n","\n","    split_pattern = f\"{pattern_1}|{pattern_2}|{pattern_3}|{pattern_4}|{pattern_5}\"\n","\n","    # -------- start processing ticket data\n","\n","    tickets = data[\"Ticket id\"].value_counts()\n","\n","    for t in tickets.index:\n","        #print(t)\n","        df = data.loc[data['Ticket id'] == t,]\n","\n","        # for one ticket content data\n","        ic_set = set([])\n","        ic_deduplicated = []\n","        for ic in df[Config.INTERACTION_CONTENT]:\n","\n","            # print(ic)\n","\n","            ic_r = re.split(split_pattern, ic)\n","            # ic_r = sum(ic_r, [])\n","\n","            ic_r = [i for i in ic_r if i is not None]\n","\n","            # replace split patterns\n","            ic_r = [re.sub(split_pattern, \"\", i.strip()) for i in ic_r]\n","\n","            # replace customer template\n","            ic_r = [re.sub(cu_pattern, \"\", i.strip()) for i in ic_r]\n","\n","            ic_current = []\n","            for i in ic_r:\n","                if len(i) > 0:\n","                    # print(i)\n","                    if i not in ic_set:\n","                        ic_set.add(i)\n","                        i = i + \"\\n\"\n","                        ic_current = ic_current + [i]\n","\n","            #print(ic_current)\n","            ic_deduplicated = ic_deduplicated + [' '.join(ic_current)]\n","        data.loc[data[\"Ticket id\"] == t, \"ic_deduplicated\"] = ic_deduplicated\n","    data.to_csv('out.csv')\n","    data[Config.INTERACTION_CONTENT] = data['ic_deduplicated']\n","    data = data.drop(columns=['ic_deduplicated'])\n","    return data\n","\n","def noise_remover(df: pd.DataFrame):\n","    noise = \"(sv\\s*:)|(wg\\s*:)|(ynt\\s*:)|(fw(d)?\\s*:)|(r\\s*:)|(re\\s*:)|(\\[|\\])|(aspiegel support issue submit)|(null)|(nan)|((bonus place my )?support.pt 自动回复:)\"\n","    df[Config.TICKET_SUMMARY] = df[Config.TICKET_SUMMARY].str.lower().replace(noise, \" \", regex=True).replace(r'\\s+', ' ', regex=True).str.strip()\n","    df[Config.INTERACTION_CONTENT] = df[Config.INTERACTION_CONTENT].str.lower()\n","    noise_1 = [\n","        \"(from :)|(subject :)|(sent :)|(r\\s*:)|(re\\s*:)\",\n","        \"(january|february|march|april|may|june|july|august|september|october|november|december)\",\n","        \"(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)\",\n","        \"(monday|tuesday|wednesday|thursday|friday|saturday|sunday)\",\n","        \"\\d{2}(:|.)\\d{2}\",\n","        \"(xxxxx@xxxx\\.com)|(\\*{5}\\([a-z]+\\))\",\n","        \"dear ((customer)|(user))\",\n","        \"dear\",\n","        \"(hello)|(hallo)|(hi )|(hi there)\",\n","        \"good morning\",\n","        \"thank you for your patience ((during (our)? investigation)|(and cooperation))?\",\n","        \"thank you for contacting us\",\n","        \"thank you for your availability\",\n","        \"thank you for providing us this information\",\n","        \"thank you for contacting\",\n","        \"thank you for reaching us (back)?\",\n","        \"thank you for patience\",\n","        \"thank you for (your)? reply\",\n","        \"thank you for (your)? response\",\n","        \"thank you for (your)? cooperation\",\n","        \"thank you for providing us with more information\",\n","        \"thank you very kindly\",\n","        \"thank you( very much)?\",\n","        \"i would like to follow up on the case you raised on the date\",\n","        \"i will do my very best to assist you\"\n","        \"in order to give you the best solution\",\n","        \"could you please clarify your request with following information:\"\n","        \"in this matter\",\n","        \"we hope you(( are)|('re)) doing ((fine)|(well))\",\n","        \"i would like to follow up on the case you raised on\",\n","        \"we apologize for the inconvenience\",\n","        \"sent from my huawei (cell )?phone\",\n","        \"original message\",\n","        \"customer support team\",\n","        \"(aspiegel )?se is a company incorporated under the laws of ireland with its headquarters in dublin, ireland.\",\n","        \"(aspiegel )?se is the provider of huawei mobile services to huawei and honor device owners in\",\n","        \"canada, australia, new zealand and other countries\",\n","        \"\\d+\",\n","        \"[^0-9a-zA-Z]+\",\n","        \"(\\s|^).(\\s|$)\"]\n","    for noise in noise_1:\n","        #print(noise)\n","        df[Config.INTERACTION_CONTENT] = df[Config.INTERACTION_CONTENT].replace(noise, \" \", regex=True)\n","    df[Config.INTERACTION_CONTENT] = df[Config.INTERACTION_CONTENT].replace(r'\\s+', ' ', regex=True).str.strip()\n","    #print(df.y1.value_counts())\n","    good_y1 = df.y1.value_counts()[df.y1.value_counts() > 10].index\n","    df = df.loc[df.y1.isin(good_y1)]\n","    #print(df.shape)\n","    return df\n","\n","def translate_to_en(texts:list[str]):\n","    import stanza\n","    from stanza.pipeline.core import DownloadMethod\n","    from transformers import pipeline\n","    from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n","\n","    t2t_m = \"facebook/m2m100_418M\"\n","    t2t_pipe = pipeline(task='text2text-generation', model=t2t_m)\n","\n","    model = M2M100ForConditionalGeneration.from_pretrained(t2t_m)\n","    tokenizer = M2M100Tokenizer.from_pretrained(t2t_m)\n","\n","    nlp_stanza = stanza.Pipeline(lang=\"multilingual\", processors=\"langid\",\n","                                 download_method=DownloadMethod.REUSE_RESOURCES)\n","    text_en_l = []\n","    for text in texts:\n","        if text == \"\":\n","            text_en_l = text_en_l + [text]\n","            continue\n","\n","        doc = nlp_stanza(text)\n","        #print(doc.lang)\n","        if doc.lang == \"en\":\n","            text_en_l = text_en_l + [text]\n","            # print(text)\n","        else:\n","            # convert to model supported language code\n","            # https://stanfordnlp.github.io/stanza/available_models.html\n","            # https://github.com/huggingface/transformers/blob/main/src/transformers/models/m2m_100/tokenization_m2m_100.py\n","            lang = doc.lang\n","            if lang == \"fro\":  # fro = Old French\n","                lang = \"fr\"\n","            elif lang == \"la\":  # latin\n","                lang = \"it\"\n","            elif lang == \"nn\":  # Norwegian (Nynorsk)\n","                lang = \"no\"\n","            elif lang == \"kmr\":  # Kurmanji\n","                lang = \"tr\"\n","\n","            case = 2\n","\n","            if case == 1:\n","                text_en = t2t_pipe(text, forced_bos_token_id=t2t_pipe.tokenizer.get_lang_id(lang='en'))\n","                text_en = text_en[0]['generated_text']\n","            elif case == 2:\n","                tokenizer.src_lang = lang\n","                encoded_hi = tokenizer(text, return_tensors=\"pt\")\n","                generated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.get_lang_id(\"en\"))\n","                text_en = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n","                text_en = text_en[0]\n","            else:\n","                text_en = text\n","            text_en_l = text_en_l + [text_en]\n","            #print(text)\n","            #print(text_en)\n","    return text_en_l"]}]}